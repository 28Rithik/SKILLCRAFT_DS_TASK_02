
---

#

# SKILLCRAFT_DS_02

# SkillCraft Technology â€“ Data Science Internship (Task 2)
![Titanic Project](SkillCraft_Task_02.png)

Hi there!  
I'm **RITHIK**, and this repository showcases my submission for **Task 2** of the **SkillCraft Technology Data Science Internship**. In this task, I worked on the **Titanic Dataset** to perform **Data Cleaning**, **Exploratory Data Analysis (EDA)**, and **Machine Learning Modeling** to uncover insights and build predictive models.

---

## ğŸŒ About the Dataset

The dataset used in this project is the famous [**Titanic Dataset**](https://www.kaggle.com/c/titanic/data), which contains details about passengers aboard the RMS Titanic. Key features include:

- PassengerId
  
- Survived (Target variable)
  
- Pclass (Ticket class)
  
- Name
  
- Sex
  
- Age
  
- SibSp (Siblings/Spouses aboard)
  
- Parch (Parents/Children aboard)
  
- Fare
  
- Embarked (Port of embarkation)  
  ... and more.
  

---

## ğŸ›  Tech Stack

  
  
  
  
  
  

---

## ğŸ” Project Workflow

### **1. Data Exploration**

- Loaded the dataset using `pandas`
  
- Checked data structure, column types, and missing values
  

### **2. Data Cleaning**

- Dropped irrelevant columns (`Ticket`, `Cabin`)
  
- Filled missing values in `Age` and `Embarked`
  
- Converted categorical features for model compatibility
  

### **3. Exploratory Data Analysis (EDA)**

Created meaningful visualizations:

- **Survival Distribution** â€“ Bar charts, pie charts
  
- **Feature Comparison** â€“ Count plots for `Sex`, `Pclass`, and `Embarked` vs `Survived`
  
- **Numerical Distributions** â€“ Histograms and boxplots for `Age`, `Fare`, `Parch`
  
- **Correlation Analysis** â€“ Heatmap of numerical features
  

### **4. Machine Learning Modeling**

Built and evaluated multiple models:

- **Logistic Regression**
  
- **Naive Bayes**
  
- **Decision Tree**
  
- **Support Vector Machine (SVM)**
  
- **K-Nearest Neighbors (KNN)**
  

Evaluation was done using **accuracy score** and **confusion matrix**.

---

## ğŸ’¡ Key Insights

âœ” **Gender Influence**: Females had a higher survival rate than males  
âœ” **Class Impact**: 1st class passengers had better survival chances  
âœ” **Embarkation Factor**: Passengers from Cherbourg (C) had higher survival rates  
âœ” **Best Model**: Naive Bayes achieved the highest accuracy among the models tested

---

## ğŸ“Š Model Performance Summary

| Model | Accuracy Score |
| --- | --- |
| **Naive Bayes** | 0.76 |
| Logistic Regression | 0.75 |
| Decision Tree | 0.74 |
| Support Vector Machine | 0.66 |
| K-Nearest Neighbors | 0.66 |

---

## ğŸ¯ Conclusion

This task was an excellent opportunity to work on a real-world dataset, perform **data preprocessing**, visualize insights through **EDA**, and build **predictive models**. It highlighted the importance of **feature engineering** and **model selection** in machine learning workflows.

---

âœ… **Suggestion for header image:** Use an image like this at the top for a professional look:  

---

ğŸ”— Connect with Me LinkedIn: Rithik CA

Portfolio: (Coming Soon)

â­ If you found this project helpful, consider giving it a star on GitHub! ğŸ”— Author: Rithik CA ğŸ’» Tech Stack: Python â€¢ Pandas â€¢ Seaborn â€¢ Matplotlib â€¢ Jupyter Notebook ğŸ“¬ Let's Connect:Â [Connect with me on LinkedIn](https://github.com/28Rithik/SKILLCRAFT_DS_TASK_01/blob/main/www.linkedin.com/in/rithik-ca-a39b02292)Â | Portfolio
